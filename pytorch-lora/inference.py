# inference.py
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
from utils import create_prompt_for_query

# LOAD ENV VARIABLES
from dotenv import load_dotenv
load_dotenv()

# --- 1. Configuration ---
model_dir = os.getenv("MERGED_MODEL_OUTPUT_DIRECTORY", "intent-classifier-final")
# The user's query you want to test
user_query = "play Room for Squares by John Mayer."

# --- 2. Load Model and Tokenizer ---
print(f"Loading model from: {model_dir}")
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=torch.float16,
    device_map="auto"
)
print(f"Model successfully loaded on device: {model.device}")

# --- 3. Create Pipeline and Run Inference ---
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    # device=model.device
)

# Set generation parameters for deterministic output
generation_kwargs = {
    "max_new_tokens": 250,
    "do_sample": False,
    "eos_token_id": [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|im_end|>")],
    "pad_token_id": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
}

final_prompt = create_prompt_for_query(user_query)
outputs = pipe(final_prompt, **generation_kwargs)

print("\n--- Sending Prompt to Model ---")

# --- 4. Process and Print the Output ---
print("\n--- Model Response ---")
generated_text = outputs[0]['generated_text']

# Extract only the content generated by the assistant
assistant_start_marker = "<|im_start|>assistant"
response_text = generated_text.split(assistant_start_marker)[-1].strip()

# Clean up any special tokens that might have been left
if "<|im_end|>" in response_text:
    response_text = response_text.split("<|im_end|>")[0].strip()

print(f"Raw response:\n{response_text}")

# Attempt to parse the JSON response for cleaner output
try:
    # Handle markdown code blocks
    if response_text.startswith("```json"):
        response_text = response_text[7:-3].strip()
        
    parsed_json = json.loads(response_text)
    print("\nParsed JSON output:")
    # Pretty-print the JSON
    print(json.dumps(parsed_json, indent=2, ensure_ascii=False))
except (json.JSONDecodeError, IndexError):
    print("\nWarning: Could not parse the model's response as valid JSON.")
    print("This may happen if the model did not follow instructions perfectly.")